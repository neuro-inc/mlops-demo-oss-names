kind: live
title: MLOps Demo - Names

defaults:
  env:
    PACHY_URI:  api.pachyderm.mlops.neu.ro:30650
    MLFLOW_URI: https://live-mlflow-server--alexeynaiden.jobs.onprem-poc.org.neu.ro

jobs:
  create_pipelines:
    name: $[[ flow.flow_id ]]-create-pipeline
    image: $[[ images.train.ref ]]
    volumes:
      - $[[ upload(volumes.project).ref_ro ]]
      - $[[ upload(volumes.config).ref_ro ]]
    env:
      NEURO_CONFIG: secret:platform-config-names
      PACHY_TOKEN: secret:pachyderm-token
    bash: |
      # Init pachctl
      pachctl config update context default --pachd-address ${PACHY_URI}
      pachctl config set active-context default
      echo "$PACHY_TOKEN" | pachctl auth use-auth-token

      # Delete pipelines if they exist
      pachctl delete pipeline train >/dev/null
      pachctl delete pipeline preprocess >/dev/null

      # Create pipeline preprocess
      pachctl create pipeline -f $[[volumes.project.mount]]/config/pipelines/preprocess.json

      # Create pipeline train
      export LIVE_B64=$(cat $[[volumes.project.mount]]/.neuro/live.yml | base64 -w 0)
      export PROJECT_B64=$(cat $[[volumes.project.mount]]/.neuro/project.yml | base64 -w 0)
      cp $[[volumes.project.mount]]/config/pipelines/train.json /tmp/train-template.json
      envsubst '$NEURO_CONFIG $MLFLOW_URI $PACHY_URI' < /tmp/train-template.json > /tmp/train.json
      sed -i -e s/##LIVE_B64##/${LIVE_B64}/ /tmp/train.json
      sed -i -e s/##PROJECT_B64##/${PROJECT_B64}/ /tmp/train.json

      pachctl create pipeline -f /tmp/train.json

      echo "Pipelines created successfully."

  train:
    image: $[[ images.train.ref ]]
    detach: False
    life_span: 10d
    volumes:
      - $[[ volumes.code.ref_ro ]]
      - $[[ volumes.config.ref_ro ]]
      - $[[ volumes.results.ref_rw ]]
      - $[[ volumes.project.ref_rw ]]
      - ${{ params.mlflow_storage }}:/usr/local/share/mlruns:rw
    env:
      PYTHONPATH: $[[ volumes.code.mount ]]
      PACHY_URI: ${{ params.pachy_uri }}
      PACHY_REPO: preprocess
      PACHY_BRANCH: master
      MLFLOW_TRACKING_URI: ${{ params.mlflow_uri }}
      PACHY_TOKEN: secret:/${{ project.owner }}/pachyderm-token
    params:
      train_iterations: "1000"
      mlflow_storage: storage:/$[[ project.owner ]]/${{ flow.project_id }}/mlruns
      mlflow_uri: ~
      pachy_uri: ~
    bash: |
        DATA_PATH=/pfs-data
        cd $[[ volumes.project.mount ]]
        mkdir -p "$DATA_PATH"

        # Init pachctl
        pachctl config update context default --pachd-address ${PACHY_URI}
        pachctl config set active-context default
        echo "$PACHY_TOKEN" | pachctl auth use-auth-token

        # Get data
        pachctl get file ${PACHY_REPO}@${PACHY_BRANCH}:/ -r -o "$DATA_PATH" | tee

        # Run training
        python -u $[[ volumes.code.mount ]]/char_rnn_classification_tutorial.py \
          --dump_dir $[[ volumes.results.mount ]] --data_path "$DATA_PATH" --n_iters $[[ params.train_iterations ]]

  jupyter:
    action: gh:neuro-actions/jupyter@v1.0.0
    args:
      image: $[[ images.train.ref ]]
      preset: cpu-small
      multi_args: $[[ multi.args ]]
      volumes_data_remote: $[[ upload(volumes.data).remote ]]
      volumes_code_remote: $[[ upload(volumes.code).remote ]]
      volumes_config_remote: $[[ upload(volumes.config).remote ]]
      volumes_notebooks_remote: $[[ upload(volumes.notebooks).remote ]]
      volumes_results_remote: $[[ volumes.results.remote ]]

  filebrowser:
    action: gh:neuro-actions/filebrowser@v1.0.1
    args:
      volumes_project_remote: $[[ volumes.project.remote ]]

  postgres:
    image: image:postgres:12.5
    name: $[[ flow.flow_id ]]-postgres
    preset: cpu-small
    http_port: 5432
    http_auth: False
    life_span: 30d
    detach: True
    volumes:
      - disk:mlops-demo-oss-dogs-postgres:/var/lib/postgresql/data:rw
    env:
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: ""
      PGDATA: /var/lib/postgresql/data/pgdata

  mlflow_server:
    image: neuromation/mlflow:1.11.0
    name: $[[ flow.flow_id ]]-mlflow-server
    preset: cpu-small
    http_port: 5000
    http_auth: False
    browse: True
    life_span: 30d
    detach: True
    volumes:
      - storage:/$[[project.owner]]/${{ flow.project_id }}/mlruns:/usr/local/share/mlruns
    cmd: |
      server --host 0.0.0.0
        --backend-store-uri=postgresql://postgres:password@${{ inspect_job('postgres').internal_hostname_named }}:5432
        --default-artifact-root=/usr/local/share/mlruns
